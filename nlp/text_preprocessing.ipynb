{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPvxWRtB2vbZfn8PkiIfb+J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swethag04/ml-projects/blob/main/nlp/text_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "-8THfPFE2rL0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qgqX60ZRwhXy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf33c8c4-5b54-4725-85f5-bb8de71ae8a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of characters:  20479\n"
          ]
        }
      ],
      "source": [
        "# reading a short story\n",
        "with open('/content/sample_data/verdict.txt','r') as f:\n",
        "  raw_text = f.read()\n",
        "print(\"Total number of characters: \", len(raw_text))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(raw_text[:99])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HuQ2GHWo2094",
        "outputId": "d3422a1d-f001-407b-c5dd-45529de30d97"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# splitting on punctutations so words and punctuations are separate list entries\n",
        "preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', raw_text)\n",
        "\n",
        "#removing white spaces\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "print(len(preprocessed))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CWAg7vfTxFBF",
        "outputId": "aea1d921-adea-4724-882d-6bbbd2eb0375"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4649\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(preprocessed[:50])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKx_HixEyw0_",
        "outputId": "39a68f1b-e123-4a7d-c454-620a7630f3c3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in', 'the', 'height', 'of', 'his', 'glory', ',', 'he', 'had', 'dropped', 'his', 'painting', ',', 'married', 'a', 'rich', 'widow', ',', 'and', 'established', 'himself']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Converting tokens into token IDs** <br>\n",
        "The next step is to convert the tokens from a string to an integer representations called token-IDs. This is an intermediate step before converting token IDs to embeddings. <br><br>\n",
        "To do this, the tokens are sorted alphabetically and duplicate tokens are removed. The unique tokens are then mapped to a unique integer value.\n",
        "\n"
      ],
      "metadata": {
        "id": "CtIm-j5i4dBY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create vocab from unieuw tokens\n",
        "all_words = sorted(list(set(preprocessed)))\n",
        "vocab_size = len(all_words)\n",
        "print(f\"Vocab size: {vocab_size}\")"
      ],
      "metadata": {
        "id": "ZLl0IB9Rxgoh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "645dc37e-2af5-4180-e42b-28c1946099d7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 1159\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print 15 entries from vocab\n",
        "vocab = {token:integer for integer , token in enumerate(all_words)}\n",
        "for i, item in enumerate(vocab.items()):\n",
        "  print(item)\n",
        "  if i>15:\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFznYwaJ5g1Q",
        "outputId": "d5873e6d-8f05-4588-f6fd-c146d5dd5367"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('!', 0)\n",
            "('\"', 1)\n",
            "(\"'\", 2)\n",
            "('(', 3)\n",
            "(')', 4)\n",
            "(',', 5)\n",
            "('--', 6)\n",
            "('.', 7)\n",
            "(':', 8)\n",
            "(';', 9)\n",
            "('?', 10)\n",
            "('A', 11)\n",
            "('Ah', 12)\n",
            "('Among', 13)\n",
            "('And', 14)\n",
            "('Are', 15)\n",
            "('Arrt', 16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing a simple text tokenizer\n",
        "class SimpleTokenizerV1:\n",
        "  def __init__(self, vocab):\n",
        "    self.str_to_int = vocab\n",
        "    # Inverse vocab\n",
        "    self.int_to_str = {i:s for s,i in vocab.items()}\n",
        "\n",
        "  def encode(self, text):\n",
        "    \"\"\" Process input text into token IDs\"\"\"\n",
        "    preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
        "    preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "    ids = [self.str_to_int[s] for s in preprocessed]\n",
        "    return ids\n",
        "\n",
        "  def decode(self, ids):\n",
        "    \"\"\"Convert token ids back to text\"\"\"\n",
        "    text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "    # replace spaces before the specified punctuation\n",
        "    text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "-KzJadUY53zS"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate a new tokenizer object\n",
        "tokenizer = SimpleTokenizerV1(vocab)\n",
        "\n",
        "text = \"\"\"It's  the last he painted, you know, Mrs. Gisburn said\n",
        "          with pardonable pride.\"\"\"\n",
        "ids = tokenizer.encode(text)\n",
        "print(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GlGEHeYp-JIg",
        "outputId": "e42ebd11-83af-48af-a01e-ee19bd6962a6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[58, 2, 872, 1013, 615, 541, 763, 5, 1155, 608, 5, 69, 7, 39, 873, 1136, 773, 812, 7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "KxGdO0g1-cJp",
        "outputId": "c9af306a-e9f1-455c-cbe4-fb8c93b8dbd9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"It' s the last he painted, you know, Mrs. Gisburn said with pardonable pride.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using tokenizer on a new text\n",
        "text = \" Hello, do you like tea?\"\n",
        "tokenizer.encode(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "ZlzAgJzB-olS",
        "outputId": "93ad5bce-3678-4c11-83c3-c77cb69634f7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'Hello'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-ca83f109ef99>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Using tokenizer on a new text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" Hello, do you like tea?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-90c663be22a4>\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mpreprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'([,.?_!\"()\\']|--|\\s)'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mpreprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr_to_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-90c663be22a4>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mpreprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'([,.?_!\"()\\']|--|\\s)'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mpreprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr_to_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Hello'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The error occured as the word \"Hello\" was not in the vocab. Our current tokenizer needs to be modified to handle unknown words.\n",
        "\n",
        "We add an <|unk|> token to represent new and unknown words that were not part of the training data and hence not part of the vocab. Also, we add an <|endoftext|> token that we can use to separate two unrelated text sources. This helps LLM understand that although they are trained on multiple independent documents and the text sources are concatenated for training, they are infact unrelated.\n",
        "\n"
      ],
      "metadata": {
        "id": "NqUhl6hd_Err"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Modifying vocab to include the 2 special tokens\n",
        "all_tokens = sorted(list(set(preprocessed)))\n",
        "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
        "vocab = {token:integer for integer, token in enumerate(all_tokens)}\n",
        "print(len(vocab.items()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCyPKF5W-6Yf",
        "outputId": "06b0226c-6ae5-41f4-ddcf-cf504a28f5fe"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1161\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, item in enumerate(list(vocab.items())[-5:]):\n",
        "  print(item)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "982Rz-iEEOL3",
        "outputId": "d2800c22-6940-43e7-f525-2a1e6268e335"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('younger', 1156)\n",
            "('your', 1157)\n",
            "('yourself', 1158)\n",
            "('<|endoftext|>', 1159)\n",
            "('<|unk|>', 1160)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizer V2\n",
        "class SimpleTokenizerV2:\n",
        "  def __init__(self, vocab):\n",
        "    self.str_to_int = vocab\n",
        "    # Inverse vocab\n",
        "    self.int_to_str = {i:s for s,i in vocab.items()}\n",
        "\n",
        "  def encode(self, text):\n",
        "    \"\"\" Process input text into token IDs\"\"\"\n",
        "    preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
        "    preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "    preprocessed = [item if item in self.str_to_int\n",
        "                    else \"<|unk|>\" for item in preprocessed ]\n",
        "    ids = [self.str_to_int[s] for s in preprocessed]\n",
        "    return ids\n",
        "\n",
        "  def decode(self, ids):\n",
        "    \"\"\"Convert token ids back to text\"\"\"\n",
        "    text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "    # replace spaces before the specified punctuation\n",
        "    text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "vAbsymS7ESuK"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text1 = \"Hello, do you like tea?\"\n",
        "text2 = \"In the sunlit terraces of the palace.\"\n",
        "text = \"<|endoftext|> \".join((text1, text2))\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6RSwueoEyW_",
        "outputId": "599c8f26-54bd-4fc1-fde1-f79507c2882b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, do you like tea?<|endoftext|> In the sunlit terraces of the palace.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleTokenizerV2(vocab)\n",
        "print(tokenizer.encode(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmthO6MvFFU8",
        "outputId": "fc523c8a-2ae7-4f63-cb0b-85d6e0c2f2bc"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1160, 5, 362, 1155, 642, 1000, 10, 1159, 57, 1013, 981, 1009, 738, 1013, 1160, 7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The list of tokenids contains 1159 for the <|endoftext|> spearator token as well as two 1160 tokens, which are used for unknown words"
      ],
      "metadata": {
        "id": "AdpWZOm7FWcb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(tokenizer.encode(text)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qj0L1ds5FNHq",
        "outputId": "9a7bfe8e-7c93-4dd4-eec7-2cd30e045225"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Byte pair encoding** <br>\n",
        "The BPE tokenizer was used to train LLMs such as GPT-3. It can handle any unknown word. It breaks down words that are not in its predefined vocabulary into smaller subword units or even individual characters, enabling it to handle out of vocabulary words. This it can parse any word and does not need to replace unknown words with special tokens such as <|unk|> <br> <br>\n",
        "Implementing BPE is relatively complicated, so we will use an existing open source librarycalled tiktoken, which implements the BPE algorithm very efficiently.\n"
      ],
      "metadata": {
        "id": "u3UrIS-5GSzN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install tiktoken"
      ],
      "metadata": {
        "id": "DfkR7F2FHO_x"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "import tiktoken\n",
        "print(f\"tiktoken version: {importlib.metadata.version('tiktoken')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hbboOC8BGSOf",
        "outputId": "d0bc4c66-c1c2-45a5-cf6c-2b6f9b145e3d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tiktoken version: 0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate BPE tokenizer from tiktoken\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ],
      "metadata": {
        "id": "lu30SH-oFlwI"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# User BPE tokenizer to encode the text\n",
        "text = \"Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.\"\n",
        "integers = tokenizer.encode(text, allowed_special = {\"<|endoftext|>\"})\n",
        "print(integers)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFv4IF2YHNVc",
        "outputId": "8f5bf581-b6fc-4c00-a436-2d5c7d8706e5"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 286, 617, 34680, 27271, 13]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "strings = tokenizer.decode(integers)\n",
        "print(strings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zvDHosOH2tq",
        "outputId": "52bdb5b0-79af-485a-807c-e5c433ee77aa"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the BPE token ids, we can see that <|endoftext|> is assigned a relatively large token ID. Also, the BPE tokenizer encodes and decodes unknown words such as \"someunknownPlace\" correctly."
      ],
      "metadata": {
        "id": "YFBxDRKFJflx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the whole verdict story\n",
        "with open('/content/sample_data/verdict.txt', 'r', encoding='utf-8') as f:\n",
        "  raw_text = f.read()\n",
        "\n",
        "enc_text = tokenizer.encode(raw_text)\n",
        "print(len(enc_text))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EsYeUvAhH-Nn",
        "outputId": "eb9bfa84-3e76-4c1f-e42a-6543c07751a4"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "enc_sample = enc_text[50:]"
      ],
      "metadata": {
        "id": "sP6ReKXXL_-I"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_size = 4\n",
        "x = enc_sample[:context_size]\n",
        "y = enc_sample[1:context_size+1]\n",
        "print(f\"x: {x}\")\n",
        "print(f\"y:       {y}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AW8_QwefMO5B",
        "outputId": "4f93af4b-5ddd-4147-8e88-3eb8390a2f2a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x: [290, 4920, 2241, 287]\n",
            "y:       [4920, 2241, 287, 257]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1, context_size+1):\n",
        "  context = enc_sample[:i]\n",
        "  desired = enc_sample[i]\n",
        "  print(context, \"---->\", desired)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRPaV-wPMpHX",
        "outputId": "e2f2bbe0-eadb-4151-9b75-93dc5e05d23b"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[290] ----> 4920\n",
            "[290, 4920] ----> 2241\n",
            "[290, 4920, 2241] ----> 287\n",
            "[290, 4920, 2241, 287] ----> 257\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1, context_size+1):\n",
        "  context = enc_sample[:i]\n",
        "  desired = enc_sample[i]\n",
        "  print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKqHfsDbNm5n",
        "outputId": "0b1a8d51-4376-4b98-edc2-17f8f547f0eb"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " and ---->  established\n",
            " and established ---->  himself\n",
            " and established himself ---->  in\n",
            " and established himself in ---->  a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing a data loader\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class GPTDatasetV1(Dataset):\n",
        "  def __init__(self, txt, tokenizer, max_length, stride):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.input_ids = []\n",
        "    self.target_ids = []\n",
        "\n",
        "    # tokenizer the entire text\n",
        "    token_ids = tokenizer.encode(txt)\n",
        "\n",
        "    # Use sliding window to chunk the text into overlapping sequences of max length\n",
        "    for i in range(0, len(token_ids) - max_length, stride):\n",
        "      input_chunk = token_ids[i:i+max_length]\n",
        "      target_chunk = token_ids[i+1:i+max_length+1]\n",
        "      self.input_ids.append(torch.tensor(input_chunk))\n",
        "      self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "  def __len__(self):\n",
        "    \"\"\"return the total number of rows in the dataset\"\"\"\n",
        "    return len(self.input_ids)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    \"\"\" return single row from dataset\"\"\"\n",
        "    return self.input_ids[idx], self.target_ids[idx]\n"
      ],
      "metadata": {
        "id": "RKxM5d4GPFae"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A data loader to generate batches with input with pairs\n",
        "def create_dataloader(txt, batch_size=4,\n",
        "                      max_length=256, stride=128, shuffle=True):\n",
        "  tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "  dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
        "  dataloader = DataLoader(dataset,\n",
        "                          batch_size = batch_size,\n",
        "                          shuffle=shuffle)\n",
        "  return dataloader\n",
        ""
      ],
      "metadata": {
        "id": "Ld7yjm0PWm9u"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open ('/content/sample_data/verdict.txt', 'r', encoding='utf-8') as f:\n",
        "  raw_text = f.read()"
      ],
      "metadata": {
        "id": "iouzL7qBYoWp"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = create_dataloader(\n",
        "                        raw_text,\n",
        "                        batch_size=1,\n",
        "                        max_length=4, #tokens in each tensor\n",
        "                        stride=1, # stride indicates the #positions the input shifts across batches\n",
        "                        shuffle=False)\n",
        "data_iter = iter(dataloader)\n",
        "first_batch = next(data_iter)\n",
        "print(first_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IV1uuxyipnFS",
        "outputId": "9871264a-4b69-4c0a-dd1b-04d5df79b234"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `first_batch` cntains two tensors: first tensor stores input token ids and second tensor stores the target token ids. Since the `max_length=4`, each of the 2 sensors contains 4 token ids."
      ],
      "metadata": {
        "id": "KP1-u8iRnPKY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "second_batch = next(data_iter)\n",
        "print(second_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1NEhKV3bU_p",
        "outputId": "5b18e13e-a23a-4ae8-8207-d2b133a6a9d0"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`stride` is used to indicate the number of positions the input shifts across batches, emulating a sliding window approach"
      ],
      "metadata": {
        "id": "NEs_330HnoKS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data loader with bigger batch size\n",
        "dataloader = create_dataloader(raw_text,\n",
        "                                batch_size=8,\n",
        "                                max_length=4,\n",
        "                                stride=5)\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)\n",
        "print(\"Inputs: \\n\", inputs)\n",
        "print(\"\\nTargets: \\n\", targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8YYpTxjWyRH",
        "outputId": "d3526cc3-92b0-4a6e-d540-a10d34c06bd6"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs: \n",
            " tensor([[   13,   198,   198,     1],\n",
            "        [  550, 18459,  1068,   284],\n",
            "        [  383,  8631,  3872,   373],\n",
            "        [ 1718,   262,  1657,   832],\n",
            "        [   11,   530,  1139,  2063],\n",
            "        [  526,  1114,  9074,    13],\n",
            "        [  284,   502,   262,  4112],\n",
            "        [  262,  3211,    12, 16337]])\n",
            "\n",
            "Targets: \n",
            " tensor([[  198,   198,     1,  5574],\n",
            "        [18459,  1068,   284,  1577],\n",
            "        [ 8631,  3872,   373,    11],\n",
            "        [  262,  1657,   832, 41160],\n",
            "        [  530,  1139,  2063,   262],\n",
            "        [ 1114,  9074,    13,   402],\n",
            "        [  502,   262,  4112,   957],\n",
            "        [ 3211,    12, 16337,  1474]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating token embeddings** <br>\n",
        "The last step for preparing input text for LLM training is to convert the token ids to embedding vectors."
      ],
      "metadata": {
        "id": "ABND8x8AqNyg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = torch.tensor([5,1,3,2])\n",
        "vocab_size = 6\n",
        "output_dim = 3\n",
        "\n",
        "torch.manual_seed(123)\n",
        "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
        "print(embedding_layer.weight)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNhq7S7BqM-4",
        "outputId": "2f9cd9ea-7b3f-4ce8-c39b-1b82044ec2f2"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.3374, -0.1778, -0.1690],\n",
            "        [ 0.9178,  1.5810,  1.3010],\n",
            "        [ 1.2753, -0.2010, -0.1606],\n",
            "        [-0.4015,  0.9666, -1.1481],\n",
            "        [-1.1589,  0.3255, -0.6315],\n",
            "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_layer(torch.tensor([3])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8ryW6gbpOmF",
        "outputId": "f7f01e71-211a-44eb-9648-198cbc80133f"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The embedding layer is a look up operation that retrieves rows from the embedding layer's weight via a token id"
      ],
      "metadata": {
        "id": "JykNR4GUrQ5o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_layer(input_ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mj3H_6-0rIxR",
        "outputId": "c9172dea-4b6f-49cc-ae54-3903e3152180"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-2.8400, -0.7849, -1.4096],\n",
            "        [ 0.9178,  1.5810,  1.3010],\n",
            "        [-0.4015,  0.9666, -1.1481],\n",
            "        [ 1.2753, -0.2010, -0.1606]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The embedding layer converts a token id into the same vector representation regardless of where it is located in the input sequence. Additional positional information needs to be injected into the LLM. There are two categories of positional aware embeddings.\n",
        "* Relative positional embeddings\n",
        "* Absolute positional embeddings\n",
        "\n",
        "Absolute positional embeddings are directly associated with specific positions in a sequence. For each position in the input sequence, a unique embedding is added to the token's embedding to convey its exact location. <br>\n",
        "\n",
        "Relative positional embeddings focus on the relative positons or distance between tokens. The model learns the relationship in terms of how far apart rather than at which exact position. The advantage is that the model can generalize better to sequences of varying lengths, even if it hasn't seen such lengths during training.\n",
        "\n",
        "OpenAI GPT model uses absolute positional embeddings."
      ],
      "metadata": {
        "id": "vNol68UitVru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_dim = 256\n",
        "vocab_size = 50257\n",
        "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
      ],
      "metadata": {
        "id": "gHD33ZDsrmxN"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 4\n",
        "dataloader = create_dataloader(raw_text,\n",
        "                               batch_size =8,\n",
        "                               max_length = max_length,\n",
        "                               stride = 5,\n",
        "                               shuffle = False)\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)\n",
        "print(\"Token IDs: \\n\", inputs)\n",
        "print(\"\\n Inputs shape: \\n\", inputs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UzMmDf5MvIb8",
        "outputId": "97f5bbff-7597-4435-d81f-849a29b46096"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token IDs: \n",
            " tensor([[   40,   367,  2885,  1464],\n",
            "        [ 3619,   402,   271, 10899],\n",
            "        [  257,  7026, 15632,   438],\n",
            "        [  257,   922,  5891,  1576],\n",
            "        [  568,   340,   373,   645],\n",
            "        [ 5975,   284,   502,   284],\n",
            "        [  326,    11,   287,   262],\n",
            "        [  286,   465, 13476,    11]])\n",
            "\n",
            " Inputs shape: \n",
            " torch.Size([8, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_embeddings = token_embedding_layer(inputs)\n",
        "print(token_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pg7-b-svibn",
        "outputId": "07f7af5a-6135-4ae0-8cac-f2de304e9a71"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = max_length\n",
        "pos_embedding_layer = torch.nn.Embedding(block_size, output_dim)\n",
        "pos_embeddings = pos_embedding_layer(torch.arange(block_size))\n",
        "print(pos_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V013ehoTvwF7",
        "outputId": "6e057536-4d28-4466-bcd8-56a3b41887b9"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_embeddings = token_embeddings + pos_embeddings\n",
        "print(input_embeddings.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKPw_qt1wEIW",
        "outputId": "1248db94-a59f-4619-8156-e9393406319e"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AWRb24mIwNLf"
      },
      "execution_count": 43,
      "outputs": []
    }
  ]
}